{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding of the feature tokens in both, Transformer and EEDGNN.\n",
    "\n",
    "    args:\n",
    "        input_dim: int, dim of the feature tokens\n",
    "        layers_dim: list, list of the number of neurons of each FC layer\n",
    "\n",
    "    return:\n",
    "          MLP network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, layers_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "\n",
    "        self.layers_dim = layers_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in layers_dim[1:]:\n",
    "            self.layers.append(nn.LayerNorm(input_dim))\n",
    "            self.layers.append(nn.Linear(input_dim, i))\n",
    "            self.layers.append(nn.GELU())\n",
    "            input_dim = i\n",
    "\n",
    "    def forward(self, input_):\n",
    "        x = input_\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    Final mlp of the network, after pooling the Transformer encoder output.\n",
    "\n",
    "    args:\n",
    "        input_dim: int, dim of the feature tokens of Transformer encoder output\n",
    "        layers_dim: list, list of the number of neurons of each FC layer\n",
    "\n",
    "    return:\n",
    "          MLP network with one neuron output and Sigmoid activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, layers_dim):\n",
    "        super(mlp, self).__init__()\n",
    "        self.layers_dim = layers_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in layers_dim[1:]:\n",
    "            self.layers.append(nn.LayerNorm(input_dim))\n",
    "            self.layers.append(nn.Linear(input_dim, i))\n",
    "            self.layers.append(nn.GELU())\n",
    "            input_dim = i\n",
    "        self.layers.append(nn.Linear(i, 1))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input_):\n",
    "        x = input_\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class EEDGCNEncoder(nn.Module):\n",
    "    def __init__(self,in_channels_n,  out_channels_n,in_channels_E,out_channels_E, k,n_layers, pooling='avg'):\n",
    "        super(EEDGCNEncoder,self).__init__()\n",
    "        self.in_channels_n=in_channels_n\n",
    "        self.out_channels_n=out_channels_n\n",
    "        self.out_channels_E=out_channels_E\n",
    "        self.in_channels_E=in_channels_E\n",
    "        self.k=k\n",
    "        self.n_layers=n_layers\n",
    "        self.pooling=pooling\n",
    "        self.layers = nn.ModuleList([EdgeConvWithEdgeFeatures(self.in_channels_n, self.out_channels_n,self.out_channels_E,self.k,self.pooling) for i in range(self.n_layers)])\n",
    "        self.nH = nn.LayerNorm(self.in_channels_n)\n",
    "        self.nE = nn.LayerNorm(self.in_channels_E)\n",
    "    def forward(self,H,E):\n",
    "\n",
    "        out_H = self.nH(H)\n",
    "        out_E = self.nE(E)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out_H,out_E = layer(out_H,out_E)\n",
    "\n",
    "        return out_H,out_E\n",
    "\n",
    "class Edgeupdate(nn.Module):\n",
    "    def __init__(self, hidden_dim, dim_e, dropout_ratio=0.2):\n",
    "        super(Edgeupdate, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dim_e = dim_e\n",
    "        self.dropout = dropout_ratio\n",
    "        self.W = nn.Linear(self.hidden_dim * 2 + self.dim_e, self.dim_e)\n",
    "\n",
    "    def forward(self, edge, node1, node2):\n",
    "        \"\"\"\n",
    "        :param edge: [batch, node,node, dim_e]\n",
    "        :param node: [batch, node, node, dim]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        node = torch.cat([node1, node2], dim=-1) # [batch, node, node, dim * 2]\n",
    "        edge = self.W(torch.cat([edge, node], dim=-1))\n",
    "        return edge  # [batch, node,npde, dim_e]\n",
    "\n",
    "class EdgeConvWithEdgeFeatures(nn.Module):\n",
    "    def __init__(self, in_channels,  out_channels_n,out_channels_E, k, pooling='avg'):\n",
    "        super(EdgeConvWithEdgeFeatures, self).__init__()\n",
    "        self.k = k\n",
    "        self.pooling=pooling\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels_n = out_channels_n\n",
    "        self.out_channels_E = out_channels_E\n",
    "        self.W = nn.Linear(self.in_channels, self.out_channels_E)\n",
    "        self.highway = Edgeupdate(self.in_channels, self.out_channels_E, dropout_ratio=0.2)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * self.in_channels, self.out_channels_n, bias=False),\n",
    "            nn.BatchNorm1d(self.out_channels_n),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x,weight_adj):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input point cloud data, shape [B, N, D]\n",
    "               B - batch size, N - number of points, D - feature dimensions\n",
    "            edge_features: Input edge features, shape [B, N, k, E]\n",
    "               E - edge feature dimensions\n",
    "        Returns:\n",
    "            x_out: Updated features after EdgeConv, shape [B, N, out_channels]\n",
    "        \"\"\"\n",
    "        B, N, D = x.size()\n",
    "        _, _, _, E = weight_adj.size()\n",
    "        \n",
    "        # Step 1: Compute pairwise distance and get k-nearest neighbors\n",
    "        pairwise_dist = torch.cdist(x, x, p=2)  # [B, N, N]\n",
    "        idx = pairwise_dist.topk(k=self.k, dim=-1, largest=False)[1]  # [B, N, k]\n",
    "        \n",
    "        # Step 2: Gather neighbor features\n",
    "        neighbors = torch.gather(\n",
    "            x.unsqueeze(2).expand(-1, -1, N, -1), \n",
    "            2, \n",
    "            idx.unsqueeze(-1).expand(-1, -1, -1, D)\n",
    "        )  # [B, N, k, D]\n",
    "        \n",
    "        # Central point repeated for k neighbors: [B, N, k, D]\n",
    "        central = x.unsqueeze(2).expand(-1, -1, self.k, -1)\n",
    "        \n",
    "        # Step 3: Compute edge features\n",
    "        relative_features = neighbors - central  # [B, N, k, D]\n",
    "        combined_features = torch.cat([central, relative_features], dim=-1)  # [B, N, k, 2*D + E]\n",
    "        \n",
    "        # Step 4: Apply MLP and aggregation\n",
    "        combined_features = self.mlp(combined_features.view(-1, 2 * D))  # [B * N * k, out_channels]\n",
    "        combined_features = combined_features.view(B, N, self.k, -1)  # Reshape to [B, N, k, out_channels]\n",
    "        \n",
    "        if self.pooling == 'avg':\n",
    "            n_out = combined_features.mean(dim=2)\n",
    "        elif self.pooling == 'max':\n",
    "            n_out = combined_features.max(dim=2)[0]\n",
    "        elif self.pooling == 'sum':\n",
    "            n_out = combined_features.sum(dim=2)\n",
    "    \n",
    "        \n",
    "        node_outputs1 = n_out.unsqueeze(1).expand(B, N, N,D)\n",
    "        node_outputs2 = node_outputs1.permute(0, 2, 1, 3).contiguous()\n",
    "        edge_outputs = self.highway(weight_adj,node_outputs1,node_outputs2)\n",
    "        \n",
    "        return n_out,edge_outputs\n",
    "    \n",
    "class MultiHead_Self_Attention(nn.Module):\n",
    "    def __init__(self, embed_dim0, embed_dim, num_heads, masked=True):\n",
    "        super(MultiHead_Self_Attention, self).__init__()\n",
    "        \"\"\"\n",
    "        MultiHead self attention with interaction matrix U. \n",
    "        The diemsnion of U is (batch,num of heads, particle tokens, particle tokens)\n",
    "        \n",
    "        Args:\n",
    "            embed_dim0: int, dim of the feature tokens\n",
    "            embed_dim: int,hidden dimension, \"scaled attention\"\n",
    "            num_heads: int,number of attention heads\n",
    "            masked: polean, using of the attention mask to remove the padded points\n",
    "            \n",
    "        return:\n",
    "              1- output of attention heads, with dim (batch,particle tokens, feature tokens)\n",
    "              2- attention weights, with dim (batch, particle tokens, particle tokens)\n",
    "        \"\"\"\n",
    "        self.masked = masked\n",
    "        self.embed_dim0 = embed_dim0\n",
    "        if embed_dim % num_heads == 0:\n",
    "\n",
    "            self.embed_dim = embed_dim\n",
    "            self.num_heads = num_heads\n",
    "            self.head_dim = embed_dim // num_heads\n",
    "        else:\n",
    "            self.embed_dim = embed_dim + (num_heads - embed_dim % num_heads)\n",
    "            self.num_heads = num_heads\n",
    "            self.head_dim = self.embed_dim // num_heads\n",
    "\n",
    "        # Initialize the linear layers\n",
    "        self.q_linear = nn.Linear(self.embed_dim0, self.embed_dim, bias=False)\n",
    "        self.k_linear = nn.Linear(self.embed_dim0, self.embed_dim, bias=False)\n",
    "        self.v_linear = nn.Linear(self.embed_dim0, self.embed_dim, bias=False)\n",
    "        self.out_linear = nn.Linear(self.embed_dim, self.embed_dim0, bias=False)\n",
    "\n",
    "    def att_mask(self, input_):\n",
    "        \"\"\"\n",
    "        Function to create attention mask with 1 for unpadded points and 0 for padded points\n",
    "\n",
    "        arg1: input data set with dim (batch_size, n_particles,n_features)\n",
    "        output: mask tensor of dim (batch_size, number of heads, n_particles,n_particles)\n",
    "        \"\"\"\n",
    "\n",
    "        mask = (input_.sum(dim=-1) != 0).float()\n",
    "        mask = mask[:, :, None, :]\n",
    "        mask = mask.repeat(1, 1, mask.size(-1), 1)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, U):\n",
    "        \"\"\"\n",
    "        Computes scaled dot-product attention.\n",
    "        dim of U: batch_size x num_heads x particle_tokens x particle_tokens\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "\n",
    "        scores = (torch.matmul(Q, K.transpose(-2, -1)) + U) / torch.sqrt(\n",
    "            torch.tensor(d_k, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "        if self.masked:\n",
    "            mask = self.att_mask(Q)\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output, attn_weights\n",
    "\n",
    "    def forward(self, query, U):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(query)\n",
    "        V = self.v_linear(query)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, U)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.embed_dim)\n",
    "        )\n",
    "\n",
    "        # Apply final linear layer # W matrix\n",
    "        output = self.out_linear(attn_output)\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        h_dim=500,\n",
    "        expansion_factor=4,\n",
    "        n_heads=10,\n",
    "        masked=True,\n",
    "    ):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        To be done: \n",
    "               Here, the hidden dimension is fixed, may be we need to adopt it in the future.\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.masked = masked\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.h_dim = h_dim\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.attention = MultiHead_Self_Attention(\n",
    "            self.input_dim, self.h_dim, self.n_heads, self.masked\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(self.input_dim)\n",
    "        self.norm2 = nn.LayerNorm(self.input_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.LayerNorm(self.input_dim),\n",
    "            nn.Linear(self.input_dim, self.expansion_factor * self.input_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(self.expansion_factor * self.input_dim),\n",
    "            nn.Linear(self.expansion_factor * self.input_dim, self.input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, query, U):\n",
    "\n",
    "        attention_out, _ = self.attention(self.norm1(query), U)\n",
    "        attention_residual_out = self.norm2(attention_out) + query\n",
    "        norm1_out = self.dropout1(attention_residual_out)\n",
    "        feed_fwd_out = self.feed_forward(norm1_out)\n",
    "        feed_fwd_residual_out = feed_fwd_out + norm1_out\n",
    "        norm2_out = self.dropout2(feed_fwd_residual_out)\n",
    "\n",
    "        return norm2_out\n",
    "\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_len : length of input sequence\n",
    "        embed_dim: dimension of embedding\n",
    "        num_layers: number of encoder layers\n",
    "        expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "        n_heads: number of heads in multihead attention\n",
    "\n",
    "    Returns:\n",
    "        out: output of the encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        embed_dim=[512, 256, 128],\n",
    "        h_dim=200,\n",
    "        num_layers=2,\n",
    "        expansion_factor=4,\n",
    "        n_heads=10,\n",
    "        masked=True,\n",
    "    ):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.masked = masked\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.n_heads = n_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = Embedding(self.input_dim, self.embed_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerLayer(\n",
    "                    self.embed_dim[-1],\n",
    "                    self.h_dim,\n",
    "                    self.expansion_factor,\n",
    "                    self.n_heads,\n",
    "                    self.masked,\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        x_new = self.embed(x)\n",
    "        n = nn.LayerNorm(x_new.size(-1))\n",
    "        out = n(x_new)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, u)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class IAFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        f_dim,\n",
    "        n_particles,\n",
    "        U_features,\n",
    "        k=7,\n",
    "        n_Transformer=2,\n",
    "        n_GNNLayers = 4,\n",
    "        h_dim=200,\n",
    "        expansion_factor=4,\n",
    "        n_heads=10,\n",
    "        masked=True,\n",
    "        pooling='avg',\n",
    "        embed_dim=[128,512,128],\n",
    "        U_dim = [128,64,64,10],\n",
    "        mlp_f_dim=[128,64]):\n",
    "        super(IAFormer, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           f_dim: int, number  of the feature tokens\n",
    "           n_particles: int, number  of the particle tokens\n",
    "           U_features: int, number of the featires in the pairwise interaction matrix\n",
    "           n_Transformer: int, number of Transformer layers\n",
    "           h_dim: int, hidden dim of the Q,K and V\n",
    "           expansion_factor: int, expansion of the size of the internal MLP layers in the Transformer layers.\n",
    "           n_heads: int, number of attention heads\n",
    "           masked: boolean, to use the attention mask\n",
    "           Pooling: str, define the pooling kind, avg, max and sum\n",
    "           embed_dim: list, define the number of neurons in the MLP for features embedding\n",
    "           U_dim: list, define the number of neuron in the MLP for pairwise interaction embedding.\n",
    "                                                                      The last number must equals the number of attention heads\n",
    "           mlp_f_dim: list, define the number of neurons in the final MLP   \n",
    "         \n",
    "        return:\n",
    "                transformer netwirk with pairwise interaction matrix included.\n",
    "        \"\"\"\n",
    "        self.f_dim=f_dim\n",
    "        self.n_particles=n_particles\n",
    "        self.U_features = U_features\n",
    "        self.k = k\n",
    "        self.n_Transformer=n_Transformer\n",
    "        self.n_GNNLayers =n_GNNLayers\n",
    "        self.n_heads= n_heads\n",
    "        self.masked = masked\n",
    "        self.expansion_factor = expansion_factor\n",
    "        self.h_dim = h_dim\n",
    "        self.pooling=pooling\n",
    "        self.embed_dim=embed_dim\n",
    "        self.mlp_f_dim = mlp_f_dim\n",
    "        self.U_dim = U_dim\n",
    "        self.mlp = mlp(self.n_particles,self.mlp_f_dim)\n",
    "        self.U_embeding =Embedding(self.U_features,self.U_dim)\n",
    "        self.embed = Embedding(self.f_dim,self.embed_dim)\n",
    "        self.encoder = TransformerEncoder(self.f_dim,embed_dim=self.embed_dim,h_dim=self.h_dim, num_layers=self.n_Transformer, \n",
    "               expansion_factor=self.expansion_factor, n_heads=self.n_heads,masked=self.masked)\n",
    "        self.GNNencoder = EEDGCNEncoder(self.embed_dim[-1],self.embed_dim[-1],self.U_dim[-1],self.U_dim[-1],self.k,self.n_GNNLayers,self.pooling)     \n",
    "        self.nW = nn.Linear(self.U_dim[-1],self.n_heads)\n",
    "        self.nH = nn.LayerNorm(self.f_dim)\n",
    "        self.nE = nn.LayerNorm(self.U_features)\n",
    "    \n",
    "    def forward(self,input_T,input_E):\n",
    "        ''' \n",
    "        input_T: dim (batch, particle tokens, feature tokens)\n",
    "        input_E: dim (batch, particle tokens, particle tokens, pairwise features)\n",
    "        '''\n",
    "        inp_E = self.U_embeding(input_E)\n",
    "        inp_T = self.embed(input_T)\n",
    "        out_H,out_E = self.GNNencoder(inp_T,inp_E)\n",
    "        \n",
    "        inp_E_T= torch.permute(self.nW(out_E),(0,-1,1,2))\n",
    "        \n",
    "        Transformer_out = self.encoder(input_T,inp_E_T)\n",
    "        if self.pooling == 'avg':\n",
    "            Transformer_output = Transformer_out.mean(dim=2)\n",
    "            out_H_ = out_H.mean(dim=2)\n",
    "\n",
    "        elif self.pooling == 'max':\n",
    "            Transformer_output = Transformer_out.max(dim=2)[0]\n",
    "            out_H_ = out_H.max(dim=2)[0]\n",
    "\n",
    "        elif self.pooling == 'sum':\n",
    "            Transformer_output = Transformer_out.sum(dim=2)\n",
    "            out_H_ = out_H.sum(dim=2)\n",
    "          \n",
    "        output_c =Transformer_output+out_H_ #torch.cat((Transformer_output,out_H_),dim=-1)\n",
    "        output = self.mlp(output_c)   \n",
    "      \n",
    "        return  output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 100, 11)\n",
    "x_edge_features = torch.rand(5, 100, 100, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IAFormer(\n",
       "  (mlp): mlp(\n",
       "    (layers): ModuleList(\n",
       "      (0): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=100, out_features=128, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Linear(in_features=64, out_features=1, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (U_embeding): Embedding(\n",
       "    (layers): ModuleList(\n",
       "      (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=4, out_features=256, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (7): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (8): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (embed): Embedding(\n",
       "    (layers): ModuleList(\n",
       "      (0): LayerNorm((11,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=11, out_features=128, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (5): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embed): Embedding(\n",
       "      (layers): ModuleList(\n",
       "        (0): LayerNorm((11,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=11, out_features=128, bias=True)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (5): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerLayer(\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (attention): MultiHead_Self_Attention(\n",
       "          (q_linear): Linear(in_features=64, out_features=75, bias=False)\n",
       "          (k_linear): Linear(in_features=64, out_features=75, bias=False)\n",
       "          (v_linear): Linear(in_features=64, out_features=75, bias=False)\n",
       "          (out_linear): Linear(in_features=75, out_features=64, bias=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (GNNencoder): EEDGCNEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EdgeConvWithEdgeFeatures(\n",
       "        (W): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (highway): Edgeupdate(\n",
       "          (W): Linear(in_features=192, out_features=64, bias=True)\n",
       "        )\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (nH): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (nE): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (nW): Linear(in_features=64, out_features=15, bias=True)\n",
       "  (nH): LayerNorm((11,), eps=1e-05, elementwise_affine=True)\n",
       "  (nE): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = 11 # number of node features\n",
    "N = 100 # number of the particles in the event\n",
    "f = 4   # number of edge features\n",
    "n_Transformer=8 # Number of Transformer layers\n",
    "n_GNN = 3\n",
    "k = 7 \n",
    "expansion_factor=4 # Expansion factor of the internal MLP in the Transformer layers\n",
    "n_heads=15  # Number of attention heads\n",
    "masked=True # If mask is used\n",
    "pooling='avg' #pooling type, max, avg or sum\n",
    "embed_dim=[256,128,64] # input embedding layers \n",
    "h_dim=embed_dim[-1] # hidden dim of the scaling matrices\n",
    "U_dim = [512,256,128,64] # Embedding layers of the edge matrix\n",
    "mlp_f_dim=[512,128,64] # layers of the final MLP\n",
    "\n",
    "model = IAFormer(D,\n",
    "        N,\n",
    "        f,\n",
    "        k=k,\n",
    "        n_Transformer=n_Transformer,\n",
    "        n_GNNLayers = n_GNN,\n",
    "        h_dim=h_dim,\n",
    "        expansion_factor=expansion_factor,\n",
    "        n_heads=n_heads,\n",
    "        masked=masked,\n",
    "        pooling=pooling,\n",
    "        embed_dim=embed_dim,\n",
    "        U_dim =U_dim,\n",
    "        mlp_f_dim=mlp_f_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4507],\n",
       "        [0.4717],\n",
       "        [0.6448],\n",
       "        [0.5296],\n",
       "        [0.5503]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x, x_edge_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
