seed_everything: 42
trainer:
  max_epochs: 1
  accelerator: "gpu"
  devices: [0]
  profiler: "simple"
  #strategy: "ddp_find_unused_parameters_true"
  precision: "32-true"  # single-precision training
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        save_weights_only: false
        mode: "min"
        monitor: "val_loss"
        every_n_train_steps: 0
        every_n_epochs: 1
        train_time_interval: null
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: "val_loss"
        min_delta: 0.0
        patience: 3
        verbose: false
  logger:
    - class_path: lightning.pytorch.loggers.TensorBoardLogger
      init_args:
        save_dir: "."
        name: "toptagging_checkpoints"

model:
  class_path: models.models.JetTaggingModule
  init_args:
    D: 11 # number of node features
    N: 100 # number of the particles in the event
    f: 4   # number of edge features
    n_Transformer: 8 # Number of Transformer layers
    n_GNN: 3
    k: 7 
    expansion_factor: 4 # Expansion factor of the internal MLP in the Transformer layers
    n_heads: 15  # Number of attention heads
    masked: True # If mask is used
    pooling: 'avg' #pooling type, max, avg or sum
    embed_dim: [256,128,64] # input embedding layers 
    h_dim: 64 # hidden dim of the scaling matrices
    U_dim: [512,256,128,64] # Embedding layers of the edge matrix
    mlp_f_dim: [512,128,64] # layers of the final MLP
    lr: 0.0001
    lr_step: 5
    lr_gamma: 0.9

data:
  class_path: data.data_handling.JetTaggingDataModule
  init_args:
    data_dir: "/mnt/d/waleed/CP-Higgs/GraphTransformer/GraphTransformer/"
    file_list: ["train-graph", "test-graph", "test-graph"]
    batch_size: 256
    num_workers: 16
